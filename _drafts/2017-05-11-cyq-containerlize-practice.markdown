---
layout: post
title:  "餐饮圈后端容器化实践"
date:   "201705511_180430"
categories: draft
---

餐饮圈后端容器化实践
====
记录[餐饮圈APP](https://www.cyqapp.com)后端容器化技术选型，以及实践过程。

# 项目介绍
简单介绍一下餐饮圈项目规模，以及团队配置，用以作为技术选型和实践的参考条件。 

## 餐饮圈介绍
餐饮圈是专注于餐饮行业社交，招聘的APP。 后端采用微服务的设计思想，将不同的业务放在不同服务中。 随着业务的发展，目前后端服务有20多个。 

容器化之前，采用的是传统的`负载均衡（阿里云负载均衡） + 多台服务器（阿里云ECS） + 数据库（阿里云RDS)`模式。

## 团队规模介绍
研发团队3～5人，同时负责前端APP和后端的研发和运维。日常的开发流程采用敏捷开发的scrum方法。 

# 一个简单的目标 — 不断提升生产力
`不断提升生产力`是促使团队尝试容器化后端的主要动力。 
随着后端服务的增多，在服务管理方面投入的时间增多， 我们在注意到团队用于发布，调试和监控服务的时间在越来越多。 因为之前采用的是单一tomcat运行所有服务，导致每一个服务的变更都需要重启整个tomcat。 tomcat也占用了大量的服务器内存。 


于是，我们列出了希望提升的几个点：
- 更简化的发布方法
- 更灵活的管理服务对资源的占用
- 更高效的利用服务器资源

基于以上三点，团队开始考虑容器化后端，使用容器编排平台来管理服务。

__注意__：容器化后端，并不是解决上面问题的唯一选择。后来的实践中也渐渐体会到，容器化后端是很重大的决定，改变的是整个后端的基础架构。之所以没有过多犹豫就选择容器化方案，是因为团队内有人熟悉容器，而且后端基础架构相对简单。

# 第一张 Architecture Overview
项目后端在阿里云上，持久化存储用的全部是阿里云的服务。 数据库使用RDS， 图片等静态文件使用OSS， redis使用云数据库Redis，所以容器化过程不存在应用服务器有持久化数据的问题， 只需要保证容器平台可以顺利链接阿里云服务器即可。 

下面是第一张`架构总览`， 简单的从逻辑层面描述了容器化后的架构。
![Architecture Overview - 01](../img/Architecture-Overview-containerlize-inferstructure.jpg)

## 容器编排平台的选择
最初，在容器编排上考虑了三个平台
- Kubernetes
- Docker Swarm
- Rancher 

很多大厂用Kubernetes实现了PaaS服务， 在企业级解决方案中Kubernetes也经常被采用作为PaaS平台的基础，可以侧面体现出Kubernetes的可靠性，稳定性等优势。作为Google自己的集群管理工具的抽象和开源版本，Kubernetes有很高的呼声。

而Docker Swarm作为Docker自家出品的容器编排服务，和Docker无缝连接，实施简单，学习曲线平滑，了解docker使用的程序员可以很容掌握。刚好，阿里云容器服务采用了Docker Swarm作为基础。

Rancher相对于前两个选择，有着开箱即用的特性，提供了完整的UI控制台。在集群管理方面有多种选择，可以选择Kubernetes, Docker Swarm来做容器编排。 但是因为国内相关实践例子不多，很快就被从选项中去掉。 


# 尝试阿里云容器服务
第一个POC是在阿里云容器服务上做的，因为`阿里云容器服务`采用Docker Swarm基础， 而且提供了一套完成的UI控制界面。 借助官方提供的文档，1天内完成了三节点测试集群的搭建，并发布了几个测试服务。一切进行的很顺利，第二天开始陆续将全部服务都部署上去，并开始性能，和稳定性测试。 
阿里云容器服务架构如下：

可以看到routing容器起到了服务发现和路由转发的作用， 负载之后所有请求都会经过routing容器。 容器内是HAproxy做请求转发。
## 第一个问题
在测试过程中，遇到了第一个问题，响应时间不稳定。 有些服务第一次请求响应时间在几千毫秒到几百毫秒波动， 并不稳定。 因为请求经过负载，又经过routing容器，然后由虚拟网络层在集群内转发到提供服务的容器。 此过程，在请求到达服务容器之前都没有日志可以跟踪，始终无法知道延迟出现在哪一步。 

再后来的实施中这个问题随着增加服务容器实例的个数得到缓解，但是始终没有找问题的根本原因。

## 雪崩
压力测试过程中，集群出现了第一次雪崩，三个节点全部掉线，并且无法ssh登录。 
调查雪崩原因有两个：
- 没有限制容器可用资源，导致容器过载后瞬间吃掉系统内存 （参考阿里云文档解决）
- Tomcat官方镜像并不能很准确的计算出jvm的最大使用内存是多少，导致服务容器过载后不断重启。  （具体解决办法可参考这边文章）
 
## 结论
### 优势
- 阿里云容器服务，提供了类似Rancher的开箱即用的特性，只需要将云服务器配置到集群中就可以自动完成集群的部署。 并且可以通过控制台界面，快速完成阿里云日志服务，云监控等功能的集成。 对于没有专门运维人员的小团队，能节省很多维护时间。 

- Docker Swarm作为基础架构，开发团队学习曲线平滑，只需要掌握docker基本知识就可以上手使用。 

### 有待解决的问题
- 集群使用的etcd作为一个外部服务独立提供，对于用户不可见，也不可控。
- 请求链路不完全透明，链路跟踪有难度
- 技术支持需要在群里喊。 当然群里管理员很负责，阿里云工程师都很专业，问题基本可以很快解决。但感觉技术支持的流程还不是太规范。

# 尝试k8s集群
虽然kubernetes提供了在aws等云上的部署参考和脚本，但是对于阿里云，目前并没有集成进去。 所以，我们参考了阿里云`初扬`提供的文档，和启动配置文件搭建集群。 
对于刚刚接触Kubernetes的人来说，这很有挑战。 

依然从3节点的测试集群开始，但马上遇到了第一个遇到的就是虚拟网络层的问题， 在`经典网络`模式下始终无法在集群内联通虚拟网络。 几次尝试未果后，转移到`VPC`网络，成功建立了集群，并打通了虚拟网络。

## 只是个开始
经过两天的折腾，k8s集群搭建完成。 但是还有很多东西需要完善， 控制台UI界面，服务发现， 日志， 监控。 很显然这些都不在k8s的核心考虑中。 所有都需要借助其他开源项目来搭建，需要投入更多的人力时间去完善。 对于小团队来说，希望将k8s用于微服务架构的生产环境，挑战很大。  咨询了一些专家后，了解到在k8s上部署`Spring Cloud`是一个用于微服务的选择，但是并没有继续尝试。 

## 结论
### 优势
k8s优势很多，比如大厂都在用， 社区很活跃。 但最终并没有完整实践k8s，所以没有办法谈对这些优势的体会。

### 对于小团队来说的挑战
- 阿里云上部署需要了解很多k8s的基础组件，虚拟网络层挑战很大
- 如果希望用于生产环境，需要自行搭建靠可用架构，并且搭建控制台，服务发现， 日志等应用。 （这里有篇文章讲解高可用架构，但也没有提到微服务相关的工具如何搭建）

# 选择 —— 阿里云容器服务
经过对两个平台的POC，我们最后选择了提供了更多工具的阿里云容器服务作为容器化后端的方案。 

对于小团队来说，容器化是为了提高生产力，开始的时候选择容器平台时，我们忽略了微服务平台这个概念，将容器编排平台等同于了微服务平台。 在POC阶段，逐渐的认识到了两者的不同，微服务平台可以构建在容器编排平台之上，也可以直接在云服务器上部署。 

选择阿里云容器服务，其实是选择了一套微服务平台，并不单单是Docker Swarm。 坚持容器化后端，也是因为基于Docker的DevOps可以不局限于某种后端技术，更灵活的隔离应用运行环境和控制应用的资源使用。

## 第二张 Architecture Overview
目前实施的架构总览





